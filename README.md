
# Vertical Domain LLM Practice: Intelligent Processing of Ancient Texts
# å‚ç›´é¢†åŸŸå¤§è¯­è¨€æ¨¡å‹å®æˆ˜ â€”â€” ä»¥å¤ç±ä¿¡æ¯æ™ºèƒ½åŒ–ä¸ºä¾‹

[![Category](https://img.shields.io/badge/Category-Training%20Materials-blue)]()
[![Language](https://img.shields.io/badge/Language-Chinese-red)]()

## ğŸ“– Book Overview (æ•™æç®€ä»‹)

**"Vertical Domain LLM Practice"** is a comprehensive textbook/tutorial series launched in 2025, designed specifically for Digital Humanities researchers and students. It bridges the gap between cutting-edge Large Language Model (LLM) technologies and Classical Chinese studies.

Unlike general AI textbooks, this resource focuses on the **full lifecycle of ancient text processing**, from pre-training and fine-tuning (SFT) to Retrieval-Augmented Generation (RAG) and Agent development.

**Target Audience:** DH students, computational linguists, and historians interested in AI.

---

## ğŸ“‚ Materials Included (èµ„æ–™é¢„è§ˆ)

This repository serves as a sample showcase for the DH Awards 2025 nomination.
*   ğŸ“„ **[Textbook_Sample_and_TOC.pdf](./Textbook_Sample_and_TOC.pdf)**: Contains the detailed **Table of Contents** and one **Sample Chapter**.
*   ğŸ“Š **[Lecture_Slides_Sample.pptx](./Lecture_Slides_Sample.pptx)**: Sample teaching slides corresponding to the practical coding sessions.

---

## ğŸ“š Syllabus & Key Topics (æ•™å­¦å¤§çº²ä¸æ ¸å¿ƒå†…å®¹)

This textbook covers 11 chapters, systematically guiding learners from theory to code implementation:

*   **Chapter 1: Fundamentals**
    *   Principles of LLMs and their application in ancient texts (OCR, punctuation, translation).
*   **Chapter 2: Base Models & Inference**
    *   Using Qwen, DeepSeek, and ChatGLM for classical Chinese.
*   **Chapter 3: Prompt Engineering**
    *   Zero-shot, Few-shot, and Chain-of-Thought (CoT) prompting for DH tasks.
*   **Chapter 4: Continued Pre-training (CPT)**
    *   Constructing domain-specific datasets using *Llama-factory*.
*   **Chapter 5: Instruction Tuning (SFT)**
    *   Building "Ancient-Modern" translation instruction datasets.
*   **Chapter 6: Human Alignment (RLHF/DPO)**
    *   Aligning model outputs with scholarly values in history and philosophy.
*   **Chapter 7: Deployment**
    *   Local deployment using vLLM, Ollama, and WebUIs.
*   **Chapter 8: RAG (Retrieval-Augmented Generation)**
    *   Building knowledge bases for ancient history retrieval.
*   **Chapter 9: Agents for DH**
    *   Developing AI Agents for automated research workflows (ReACT, LangChain).
*   **Chapter 10: Evaluation**
    *   Benchmarking LLM capabilities on classical texts (C-Eval, CMMLU).
*   **Chapter 11: Multimodal LLMs**
    *   Handling ancient scripts, scanned images, and calligraphy recognition.

## ğŸ›  Tech Stack (æŠ€æœ¯æ ˆ)
*   **Frameworks:** PyTorch, Llama-factory, LangChain
*   **Models:** Qwen-2, DeepSeek, Yi, GLM-4
*   **Tools:** vLLM, ChromaDB, Faiss
